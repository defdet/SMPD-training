{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["!pip3 install transformers zstandard jsonlines peft wandb bitsandbytes lion-pytorch -q\n","!pip3 install accelerate datasets sentencepiece langchain torch_xla[tpuvm] -q\n","!pip uninstall tensorflow -y #that's the meme part"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["get_ipython().kernel.do_shutdown(True)\n","### for good measures restart kernel"]},{"cell_type":"markdown","metadata":{},"source":["**Tokens?**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!huggingface-cli login --token <hf_read_token> #for downloading gated models\n","# import wandb\n","# wandb.login()"]},{"cell_type":"markdown","metadata":{},"source":["**Sharding Module for different Architechture**"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-02-15T18:22:56.582246Z","iopub.status.busy":"2024-02-15T18:22:56.581852Z","iopub.status.idle":"2024-02-15T18:22:56.590353Z","shell.execute_reply":"2024-02-15T18:22:56.589512Z","shell.execute_reply.started":"2024-02-15T18:22:56.582213Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Overwriting spmd_util.py\n"]}],"source":["%%writefile spmd_util.py\n","import torch\n","import torch.nn as nn\n","import re\n","import torch_xla.experimental.xla_sharding as xs\n","import torch_xla.core.xla_model as xm\n","from transformers import (\n","    GPTNeoXConfig, T5Config, LlamaConfig, GPT2Config, MistralConfig, Qwen2Config, MixtralConfig, PhiConfig\n",")\n","\n","# ends with $ to prevent sharding lora parameters\n","\n","\n","T5_RULES = (\n","    # embeddings\n","    (\"shared$\", (\"mp\", \"fsdp\")),\n","    (\"embed_tokens$\", (\"mp\", \"fsdp\")),\n","    \n","    # attention\n","    (\"q$\", (\"fsdp\", \"mp\")),\n","    (\"k$\", (\"fsdp\", \"mp\")),\n","    (\"v$\", (\"fsdp\", \"mp\")),\n","    (\"o$\", (\"mp\", \"fsdp\")),\n","\n","    # mlp\n","    (\"w$\", (\"fsdp\", \"mp\")),\n","    (\"wi_0$\", (\"fsdp\", \"mp\")),\n","    (\"wi_1$\", (\"fsdp\", \"mp\")),\n","    (\"wo$\", (\"mp\", \"fsdp\")),\n","\n","    # seq2seq lm head\n","    (\"lm_head\", (\"fsdp\", \"mp\")),\n",")\n","\n","QWEN_RULES = (\n","    (\"model\\\\.embed_tokens\", (\"mp\", \"fsdp\")),\n","    (\"self_attn\\\\.(q_proj|k_proj|v_proj)\", (\"fsdp\", \"mp\")),\n","    (\"self_attn\\\\.o_proj\", (\"mp\", \"fsdp\")),\n","    (\"mlp\\\\.gate_proj\", (\"fsdp\", \"mp\")),\n","    (\"mlp\\\\.down_proj\", (\"mp\", \"fsdp\")),\n","    (\"mlp\\\\.up_proj\", (\"fsdp\", \"mp\")),\n","    (\"lm_head\", (\"fsdp\", \"mp\")),\n","    )\n","GPT2_RULES = (\n","    # embeddings\n","    (\"wte\", (\"mp\", \"fsdp\")), \n","    (\"wpe\", (\"mp\", \"fsdp\")),\n","    \n","    # attention\n","    (\"c_attn\", (\"fsdp\", \"mp\")),\n","    (\"c_proj\", (\"mp\", \"fsdp\")),\n","    \n","    # mlp\n","    (\"c_fc\", (\"fsdp\", \"mp\")), \n","    (\"c_proj\", (\"mp\", \"fsdp\")),\n","    \n","    # output \n","    (\"ln_f\", (\"fsdp\", \"mp\")),\n",")\n","MISTRAL_RULES = (\n","    (\"model\\\\.embed_tokens\", (\"mp\", \"fsdp\")),\n","    (\"self_attn\\\\.(q_proj|k_proj|v_proj)\", (\"fsdp\", \"mp\")),\n","    (\"self_attn\\\\.o_proj\", (\"mp\", \"fsdp\")),\n","    (\"mlp\\\\.gate_proj\", (\"fsdp\", \"mp\")),\n","    (\"mlp\\\\.down_proj\", (\"mp\", \"fsdp\")),\n","    (\"mlp\\\\.up_proj\", (\"fsdp\", \"mp\")),\n","    (\"lm_head\", (\"fsdp\", \"mp\")),\n","    )\n","\n","\n","PHI_RULES = (\n","    ### (regex) linear modules, (list[sharding methods]) )\n","    (\"model\\\\.embed_tokens\", (\"mp\", \"fsdp\")),\n","    (\"self_attn\\\\.(q_proj|k_proj|v_proj)\", (\"fsdp\", \"mp\")),\n","    (\"self_attn\\\\.dense\", (\"mp\", \"fsdp\")),\n","    (\"mlp\\\\.fc2\", (\"mp\", \"fsdp\")),  \n","    (\"mlp\\\\.fc1\", (\"fsdp\", \"mp\")),\n","    (\"lm_head\", (\"fsdp\", \"mp\")),\n","    \n",")\n","\n","LLAMA_RULES = (\n","    (\"model\\\\.embed_tokens\", (\"mp\", \"fsdp\")),\n","    (\"self_attn\\\\.(q_proj|k_proj|v_proj)\", (\"fsdp\", \"mp\")),\n","    (\"self_attn\\\\.o_proj\", (\"mp\", \"fsdp\")),\n","    (\"mlp\\\\.gate_proj\", (\"fsdp\", \"mp\")),\n","    (\"mlp\\\\.down_proj\", (\"mp\", \"fsdp\")),\n","    (\"mlp\\\\.up_proj\", (\"fsdp\", \"mp\")),\n","    (\"lm_head\", (\"fsdp\", \"mp\")),\n","    )\n","\n","GPTNEOX_RULES = (\n","    # embeddings\n","    (\"gpt_neox\\\\.embed_in\", (\"mp\", \"fsdp\")),\n","    # atention\n","    (\"attention\\\\.query_key_value$\", (\"fsdp\", \"mp\")),\n","    (\"attention\\\\.dense$\", (\"mp\", \"fsdp\")),\n","    # mlp\n","    (\"mlp\\\\.dense_h_to_4h$\", (\"fsdp\", \"mp\")),\n","    (\"mlp\\\\.dense_4h_to_h$\", (\"mp\", \"fsdp\")),\n","    # output\n","    (\"embed_out\", (\"fsdp\", \"mp\")),\n",")\n","\n","\n","\n","MIXTRAL_RULES = (\n","    (\"model\\\\.embed_tokens\", (\"mp\", \"fsdp\")),\n","    (\"self_attn\\\\.(q_proj|k_proj|v_proj)\", (\"fsdp\", \"mp\")),\n","    (\"self_attn\\\\.o_proj\", (\"mp\", \"fsdp\")),\n","    (\"w1\", (\"fsdp\", \"mp\")),\n","    (\"w2\", (\"mp\", \"fsdp\")),\n","    (\"w3\", (\"fsdp\", \"mp\")),\n","    (\"gate\", (\"mp\", \"fsdp\")),\n","    (\"lm_head\", (\"fsdp\", \"mp\")),\n","    )\n","\n","\n","    \n","ALL_RULES = [\n","    (GPTNeoXConfig, GPTNEOX_RULES),\n","    (T5Config, T5_RULES),\n","    (LlamaConfig, LLAMA_RULES),\n","    (GPT2Config, GPT2_RULES),\n","    (MistralConfig, MISTRAL_RULES),\n","    (Qwen2Config, QWEN_RULES),\n","    (MixtralConfig, MIXTRAL_RULES),\n","    (PhiConfig,PHI_RULES),\n","]\n","\n","def find_rule(model):\n","    for config, rule in ALL_RULES:\n","        if model.config.__class__ == config:\n","            return rule\n","    raise Exception(\"unsupported model to partitioning\")\n","\n","strkey2id = {\n","    \"dp\": 0,\n","    \"fsdp\": 1,\n","    \"mp\": 2\n","}\n","\n","def partition_module(model, mesh, device=xm.xla_device(), verbose=False):\n","    partition_specs = find_rule(model)\n","    rule = [(k, tuple([strkey2id[x] for x in v])) for k, v in partition_specs]\n","        \n","    # print(rule)\n","\n","    for name, module in model.named_modules():\n","        module.to(device)\n","        # print(name, module.__class__.__name__)\n","        if isinstance(module, (nn.Embedding, nn.Linear)):\n","            for rule_pattern, spec in rule:\n","                if re.findall(rule_pattern, name):\n","                    if verbose:\n","                        print(\"match\", rule_pattern, name)\n","                    \n","                    xs.mark_sharding(module.weight, mesh, spec)\n","                    break\n","        \n","def partition_module_dp(model, mesh, device=xm.xla_device(), verbose=True):\n","    spec = (1, 2)\n","\n","    for name, module in model.named_modules():\n","        module.to(device)\n","        if isinstance(module, (nn.Embedding, nn.Linear)):\n","            xs.mark_sharding(module.weight, mesh, spec)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["### TODO\n","\"\"\"\n","-Support PHI-2 // curent PR\n","-Support Gradient Accumulation\n","-Support Quantization\n","-Support for 8-bit optimizers and paged optimizers\n","-Support interference after sharding\n","\"\"\""]},{"cell_type":"markdown","metadata":{},"source":["**Required Libs**"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-02-15T18:22:17.425838Z","iopub.status.busy":"2024-02-15T18:22:17.425524Z","iopub.status.idle":"2024-02-15T18:22:22.575713Z","shell.execute_reply":"2024-02-15T18:22:22.574136Z","shell.execute_reply.started":"2024-02-15T18:22:17.425810Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import os\n","import pandas as pd\n","import numpy as np\n","import datasets\n","import torch.optim as optim\n","import torch_xla.debug.profiler as xp\n","import torch_xla.core.xla_model as xm\n","import torch_xla.distributed.xla_multiprocessing as xmp # We also import mp modules if we wanna use that for some reason\n","import torch_xla.distributed.parallel_loader as pl\n","import torch_xla.test.test_utils as test_utils\n","import torch\n","import torch.nn as nn\n","import re\n","import torch_xla.experimental.xla_sharding as xs\n","import torch_xla.core.xla_model as xm\n","from transformers import (\n","    GPTNeoXConfig, T5Config, LlamaConfig, AutoTokenizer, AutoModelForCausalLM, MistralConfig, Qwen2Config, GPT2Config, DataCollatorWithPadding, AutoConfig, AutoModelForSequenceClassification\n",") # You can use any of models with those configs (even flan T5 xxl!). Other models are not supported.\n","\n","from transformers import logging as hf_logging\n","import torch.nn.functional as F\n","import torch_xla.runtime as xr\n","\n","xr.use_spmd()\n","\n","import torch_xla.experimental.xla_sharding as xs # \"experimental\" prefix always means you're gonna have a good time LMAO\n","from torch_xla.experimental.xla_sharded_tensor import XLAShardedTensor\n","from torch_xla.experimental.xla_sharding import Mesh\n","\n","from peft import LoraConfig, TaskType, get_peft_model # If we wanna use peft. Quantazation requiers GPU though. You'll have to download already quantazed models\n","from spmd_util import partition_module                # You could experiment with using already quantazed models like 4bit/Llama-2-7b-Chat-GPTQ if you're feeling funny\n","from datasets import Dataset, load_dataset, concatenate_datasets\n","from dataclasses import dataclass\n","from tqdm import tqdm\n","\n","import transformers\n","import datasets\n","import pandas as pd\n","import numpy as np\n","from datasets import Dataset\n","from torch.utils.data import Dataset as TorchDataset\n","import torch.utils\n","try:\n","    !export USE_TORCH=True #If we don't do this, transformers will seemingly bork the session upon import. Really weird error.\n","    os.environ[\"PJRT_DEVICE\"] = \"TPU\"\n","    os.environ.pop('TPU_PROCESS_ADDRESSES')\n","    os.environ.pop('CLOUD_TPU_TASK_ID')\n","    hf_logging.set_verbosity_error() # It can still display warnings which is a bit annoying but whatever\n","except:\n","    pass\n"]},{"cell_type":"markdown","metadata":{},"source":["**Configuration**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["MAX_INPUT=1024\n","MODEL = \"g-ronimo/phi-2-OpenHermes-2.5\" #You should be able to use 7B model with no changes! There should be enough HBM\n","SAVED_MODEL = \"fhai50032/PHI-Coder\"\n","\n"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-02-15T18:23:53.907380Z","iopub.status.busy":"2024-02-15T18:23:53.907006Z","iopub.status.idle":"2024-02-15T18:23:54.572075Z","shell.execute_reply":"2024-02-15T18:23:54.571038Z","shell.execute_reply.started":"2024-02-15T18:23:53.907348Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["tokenizer_config.json: 100%|██████████| 8.20k/8.20k [00:00<00:00, 17.1MB/s]\n","vocab.json: 100%|██████████| 798k/798k [00:00<00:00, 10.4MB/s]\n","merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 55.5MB/s]\n","tokenizer.json: 100%|██████████| 2.12M/2.12M [00:00<00:00, 26.9MB/s]\n","added_tokens.json: 100%|██████████| 1.15k/1.15k [00:00<00:00, 6.11MB/s]\n","special_tokens_map.json: 100%|██████████| 462/462 [00:00<00:00, 2.60MB/s]"]},{"name":"stdout","output_type":"stream","text":["Tokens :\n"," {'bos_token': '<|endoftext|>', 'eos_token': '<|im_end|>', 'unk_token': '<|endoftext|>', 'pad_token': '<PAD>'} \n","\n","\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["from transformers import AutoTokenizer\n","tokenizer = AutoTokenizer.from_pretrained(MODEL)\n","if 'pad_token' not in tokenizer.special_tokens_map:\n","  tokenizer.pad_token=tokenizer.eos_token\n","\n","\n","print(f\"Tokens :\\n {tokenizer.special_tokens_map} \\n\\n\")"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-02-15T18:23:59.519364Z","iopub.status.busy":"2024-02-15T18:23:59.518971Z","iopub.status.idle":"2024-02-15T18:23:59.527452Z","shell.execute_reply":"2024-02-15T18:23:59.526628Z","shell.execute_reply.started":"2024-02-15T18:23:59.519331Z"},"trusted":true},"outputs":[],"source":["class ConversationDataset(TorchDataset):\n","    def __init__(self, tokenizer, max_length=1024, dataset=None):\n","        self.dataset = dataset\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, idx):\n","        messages = self.dataset[idx][\"conversations\"]\n","        text = \"\"\n","        for message in messages:\n","            role = message[\"from\"]\n","            if role == \"system\":\n","                text += f\"<|im_start|>system\\n{message['value']}<|im_end|>\\n\"\n","            if role == \"human\":\n","                text += f\"<|im_start|>user\\n{message['value']}<|im_end|>\\n\"\n","            if role == \"function-call\":\n","                text += f\"<|im_start|>call\\n{message['value']}<|im_end|>\\n\"\n","            if role == \"function-response\":\n","                text += f\"<|im_start|>function\\n{message['value']}<|im_end|>\\n\"\n","            if role ==\"gpt\":\n","                text += f\"<|im_start|>assistant\\n{message['value']}{self.tokenizer.eos_token}\"\n","        input_ids = self.tokenizer(text, add_special_tokens=True, max_length=self.max_length, truncation=True, padding=\"max_length\", return_attention_mask=True, return_tensors=\"pt\")\n","        return {\n","            \"input_ids\": input_ids[\"input_ids\"].squeeze(0),\n","            \"labels\": input_ids[\"input_ids\"].squeeze(0),\n","            \"mask\":input_ids[\"attention_mask\"].squeeze(0),\n","        }"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-02-15T18:24:09.860552Z","iopub.status.busy":"2024-02-15T18:24:09.860187Z","iopub.status.idle":"2024-02-15T18:24:25.763655Z","shell.execute_reply":"2024-02-15T18:24:25.762903Z","shell.execute_reply.started":"2024-02-15T18:24:09.860522Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading readme: 100%|██████████| 645/645 [00:00<00:00, 4.59MB/s]\n","Downloading data: 100%|██████████| 44.9M/44.9M [00:03<00:00, 12.9MB/s]\n","Downloading data: 100%|██████████| 45.1M/45.1M [00:03<00:00, 14.0MB/s]\n","Generating train split: 100%|██████████| 75197/75197 [00:00<00:00, 77589.77 examples/s]\n"]}],"source":["train_dataset=\"fhai50032/magicoder-oss-instruct-sharegpt-75k\"\n","test_dataset=\"fhai50032/magicoder-oss-instruct-sharegpt-75k\"\n","\n","train_data = load_dataset(train_dataset, split=\"train[0:2000]\").shuffle(seed=69)\n","val = (load_dataset(test_dataset, split=\"train[:64]\")).shuffle(seed=420)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-02-15T18:24:28.094015Z","iopub.status.busy":"2024-02-15T18:24:28.093660Z","iopub.status.idle":"2024-02-15T18:24:28.099211Z","shell.execute_reply":"2024-02-15T18:24:28.098489Z","shell.execute_reply.started":"2024-02-15T18:24:28.093972Z"},"trusted":true},"outputs":[{"data":{"text/plain":["2000"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["len(train_data)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-02-15T18:24:38.877652Z","iopub.status.busy":"2024-02-15T18:24:38.877267Z","iopub.status.idle":"2024-02-15T18:24:38.882548Z","shell.execute_reply":"2024-02-15T18:24:38.881601Z","shell.execute_reply.started":"2024-02-15T18:24:38.877618Z"},"trusted":true},"outputs":[],"source":["FLAGS = {'MAX_INPUT': MAX_INPUT,\n","         'LOGGING_STEPS': 1,\n","         'NUM_EPOCHS': 1,\n","         'BATCH_SIZE': 8, #Making batch_size lower then 8 will result in slower training, but will allow for larger models\\context. Fortunately, we have 128GBs. Setting higher batch_size doesn't seem to improve time.\n","          'LEN_DATA': len(train_data),\n","         'VAL_PER_STEP': 10,\n","#         'GRAD_ACCUMULATION':2,\n","#          'MAX_GRAD_CLIP':1.0,\n","        'LEARNING_RATE':2e-5,\n","         'WARMUP_RATIO':0.1,\n","         'OPTIMIZER':'adamw', # default = 'adamw8bit'  options->  ['adamw','adamw8bit','adafactor','lion']           \n","         'SCHEDULAR':'linear', # default= 'cosine'     options:-> ['linear','cosine']\n","         'WEIGHT_DECAY':0.0,\n","         'TRAIN_DATASET':train_dataset,\n","         \"TEST_DATASET\":test_dataset,\n","         'WANDB':True,\n","        'PROJECT':'Silver-Pulse'} # Indian pun :) "]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-02-15T18:24:40.631269Z","iopub.status.busy":"2024-02-15T18:24:40.630919Z","iopub.status.idle":"2024-02-15T18:24:40.636490Z","shell.execute_reply":"2024-02-15T18:24:40.635668Z","shell.execute_reply.started":"2024-02-15T18:24:40.631239Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'MAX_INPUT': 1024,\n"," 'LOGGING_STEPS': 1,\n"," 'NUM_EPOCHS': 1,\n"," 'BATCH_SIZE': 8,\n"," 'LEN_DATA': 2000,\n"," 'VAL_PER_STEP': 10,\n"," 'LEARNING_RATE': 2e-05,\n"," 'WARMUP_RATIO': 0.1,\n"," 'OPTIMIZER': 'adamw',\n"," 'SCHEDULAR': 'linear',\n"," 'WEIGHT_DECAY': 0.0,\n"," 'TRAIN_DATASET': 'fhai50032/magicoder-oss-instruct-sharegpt-75k',\n"," 'TEST_DATASET': 'fhai50032/magicoder-oss-instruct-sharegpt-75k',\n"," 'WANDB': True,\n"," 'PROJECT': 'Silver-Pulse'}"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["FLAGS"]},{"cell_type":"markdown","metadata":{},"source":["**Quantization When??**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# from transformers import BitsAndBytesConfig\n","\n","# bnb_config = BitsAndBytesConfig(\n","#     load_in_4bit=True,\n","#     bnb_4bit_use_double_quant=True,\n","#     bnb_4bit_quant_type=\"nf4\",\n","#     bnb_4bit_compute_dtype=torch.bfloat16,\n","#     llm_int8_has_fp16_weight=False,\n","        \n","# )\n","# model = AutoModelForCausalLM.from_pretrained(MODEL,torch_dtype=torch.bfloat16,quantization_config=bnb_config,\n","#     trust_remote_code=True,\n","#     low_cpu_mem_usage=True) "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model = AutoModelForCausalLM.from_pretrained(MODEL,torch_dtype=torch.bfloat16) \n","### use only bf16 or atleast set compute type to bf16 "]},{"cell_type":"markdown","metadata":{},"source":["**LoRA Applicable**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["ls=LoraConfig(\n","    r = 32, # Lora Rank ,I would prefer 8-32 for smaller models like 7b\n","    target_modules = ['v_proj', 'down_proj', 'up_proj', 'o_proj', 'q_proj', 'gate_proj', 'k_proj'],\n","    lora_alpha = 48, #weight_scaling\n","    lora_dropout = 0.05, # Supports any, but = 0 is optimized\n","    bias = \"none\",    # Supports any, but = \"none\" is optimize\n","    # modules_to_save = [\"lm_head\", \"embed_tokens\"] ## if you use new chat formats or embedding tokens\n",")\n","model = get_peft_model(model, ls)\n","model.print_trainable_parameters()"]},{"cell_type":"markdown","metadata":{},"source":["**Data-Distributer**"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-02-15T18:25:22.232939Z","iopub.status.busy":"2024-02-15T18:25:22.232594Z","iopub.status.idle":"2024-02-15T18:25:22.239848Z","shell.execute_reply":"2024-02-15T18:25:22.238886Z","shell.execute_reply.started":"2024-02-15T18:25:22.232909Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Max Steps:- 32  , Each Step has 64 lines\n"]}],"source":["train_data = ConversationDataset(tokenizer, dataset=train_data, max_length=1024)\n","val = ConversationDataset(tokenizer, dataset=val)\n","train_sampler = torch.utils.data.distributed.DistributedSampler(\n","    train_data, num_replicas=8, rank=xm.get_ordinal(), shuffle=True)\n","training_loader = torch.utils.data.DataLoader(train_data, batch_size=FLAGS[\"BATCH_SIZE\"], sampler=train_sampler)\n","val_sampler = torch.utils.data.distributed.DistributedSampler(\n","    val, num_replicas=8, rank=xm.get_ordinal(), shuffle=True)\n","testing_loader = torch.utils.data.DataLoader(val, batch_size=FLAGS[\"BATCH_SIZE\"], sampler=val_sampler)\n","\n","print(f\"Max Steps:- {len(training_loader)}  , Each Step has {8*FLAGS['BATCH_SIZE']} lines\")\n","\n","FLAGS['STEPS']=len(training_loader)\n","FLAGS['BATCH_DATA']=FLAGS['BATCH_SIZE']*8 ## 8 CORES ON TPU \n","# print(device)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-02-15T18:25:41.772916Z","iopub.status.busy":"2024-02-15T18:25:41.772550Z","iopub.status.idle":"2024-02-15T18:25:41.778887Z","shell.execute_reply":"2024-02-15T18:25:41.778206Z","shell.execute_reply.started":"2024-02-15T18:25:41.772884Z"},"trusted":true},"outputs":[],"source":["def get_nb_trainable_parameters(model):\n","        r\"\"\"\n","        Returns the number of trainable parameters and number of all parameters in the model.\n","        \"\"\"\n","        trainable_params = 0\n","        all_param = 0\n","        for _, param in model.named_parameters():\n","            num_params = param.numel()\n","            # if using DS Zero 3 and the weights are initialized empty\n","            if num_params == 0 and hasattr(param, \"ds_numel\"):\n","                num_params = param.ds_numel\n","\n","            # Due to the design of 4bit linear layers from bitsandbytes\n","            # one needs to multiply the number of parameters by 2 to get\n","            # the correct number of parameters\n","            if param.__class__.__name__ == \"Params4bit\":\n","                num_params = num_params * 2\n","\n","            all_param += num_params\n","            if param.requires_grad:\n","                trainable_params += num_params\n","\n","        return trainable_params, all_param\n","def print_trainable_parameters(model):\n","        \"\"\"\n","        Prints the number of trainable parameters in the model.\n","        \"\"\"\n","        trainable_params, all_param = get_nb_trainable_parameters(model)\n","        \n","        print(\n","            f\"trainable params: {trainable_params:,d} || all params: {all_param:,d} || trainable%: {100 * trainable_params / all_param}\"\n","        )"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-02-15T18:25:44.378335Z","iopub.status.busy":"2024-02-15T18:25:44.377974Z","iopub.status.idle":"2024-02-15T18:25:44.384299Z","shell.execute_reply":"2024-02-15T18:25:44.383549Z","shell.execute_reply.started":"2024-02-15T18:25:44.378305Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["trainable params: 2,779,683,840 || all params: 2,779,683,840 || trainable%: 100.0\n"]}],"source":["print_trainable_parameters(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["config = AutoConfig.from_pretrained(MODEL)\n","num_devices = xr.global_runtime_device_count()\n","mesh_shape = (1, num_devices, 1)\n","device_ids = np.array(range(num_devices))\n","mesh = Mesh(device_ids, mesh_shape, ('dp', 'fsdp', 'mp'))\n","partition_module(model, mesh) # After this, the model is sharded between cores but still has the same API as if it was on single device. Neat."]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-02-15T18:25:49.268465Z","iopub.status.busy":"2024-02-15T18:25:49.267698Z","iopub.status.idle":"2024-02-15T18:25:49.273194Z","shell.execute_reply":"2024-02-15T18:25:49.272430Z","shell.execute_reply.started":"2024-02-15T18:25:49.268432Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'MAX_INPUT': 1024,\n"," 'LOGGING_STEPS': 1,\n"," 'NUM_EPOCHS': 1,\n"," 'BATCH_SIZE': 8,\n"," 'LEN_DATA': 2000,\n"," 'VAL_PER_STEP': 10,\n"," 'LEARNING_RATE': 2e-05,\n"," 'WARMUP_RATIO': 0.1,\n"," 'OPTIMIZER': 'adamw',\n"," 'SCHEDULAR': 'linear',\n"," 'WEIGHT_DECAY': 0.0,\n"," 'TRAIN_DATASET': 'fhai50032/magicoder-oss-instruct-sharegpt-75k',\n"," 'TEST_DATASET': 'fhai50032/magicoder-oss-instruct-sharegpt-75k',\n"," 'WANDB': True,\n"," 'PROJECT': 'Silver-Pulse',\n"," 'STEPS': 32,\n"," 'BATCH_DATA': 64}"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["FLAGS"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-02-15T18:27:54.603359Z","iopub.status.busy":"2024-02-15T18:27:54.602669Z","iopub.status.idle":"2024-02-15T18:27:55.521664Z","shell.execute_reply":"2024-02-15T18:27:55.520426Z","shell.execute_reply.started":"2024-02-15T18:27:54.603323Z"},"trusted":true},"outputs":[],"source":["!export XLA_USE_BF16=1\n","import wandb\n","from transformers import AdamW,Adafactor\n","from lion_pytorch import Lion #optimizer best used for large batch size ~ 4096+\n","from transformers import get_linear_schedule_with_warmup,get_cosine_schedule_with_warmup\n","# from bitsandbytes.optim import AdamW8bit \n","\n","\n","\n","def train(FLAGS):\n","\n","    \n","    ### Configuring Training\n","    device = xm.xla_device()\n","    update_params= filter(lambda p: p.requires_grad, model.parameters())\n","    num_iterations = FLAGS[\"NUM_EPOCHS\"] * FLAGS['STEPS']  #    // FLAGS['GRAD_ACCUMULATION'])\n","    warmup_steps = int(num_iterations * FLAGS['WARMUP_RATIO'])\n","    __wandb__=FLAGS['WANDB']\n","    \n","    if __wandb__:\n","        wandb.init(project=FLAGS['PROJECT'],config=FLAGS)\n","    \n","    ### Optimizers\n","    \n","    if (FLAGS['OPTIMIZER']).lower()=='adamw':\n","        optimizer = AdamW(update_params, eps=1e-8, lr=FLAGS['LEARNING_RATE'], betas=(0.9, 0.999),weight_decay=FLAGS['WEIGHT_DECAY'],no_deprecation_warning=True)\n","    elif (FLAGS['OPTIMIZER']).lower()=='lion':\n","        optimizer = Lion(update_params, lr=FLAGS['LEARNING_RATE'],weight_decay=FLAGS['WEIGHT_DECAY'])\n","    elif (FLAGS['OPTIMIZER']).lower()=='adafactor':\n","        optimizer = Adafactor(update_params,lr=FLAGS['LEARNING_RATE'],weight_decay=FLAGS['WEIGHT_DECAY'],scale_parameter=True,relative_step=False)\n","    else:\n","#         optimizer = AdamW8bit(update_params, eps=1e-8, lr=FLAGS['LEARNING_RATE'], betas=(0.9, 0.999),weight_decay=FLAGS['WEIGHT_DECAY'])\n","        pass\n","\n","    \n","    \n","    ### Schedulars\n","    \n","    if (FLAGS['SCHEDULAR']).lower()=='linear':\n","        scheduler = get_linear_schedule_with_warmup(optimizer,warmup_steps,num_iterations)\n","    else:\n","        schedular = get_cosine_schedule_with_warmup(optimizer,warmup_steps,num_iterations)\n","        \n","        \n","    \n","    \n","    ### Training Loop\n","    \n","    for epoch in range(1, FLAGS['NUM_EPOCHS'] + 1):\n","        model.train()\n","        xm.master_print('Epoch {} train begin {}'.format(epoch, test_utils.now()))\n","        for step, batch in enumerate(training_loader):\n","            input_ids, labels,mask = batch[\"input_ids\"].to(device),  batch[\"labels\"].to(device),batch['mask'].to(device)\n","            xs.mark_sharding(input_ids, mesh, (0, 1))\n","            xs.mark_sharding( labels,   mesh, (0, 1))\n","            xs.mark_sharding(  mask,    mesh, (0, 1))\n","            outputs = model(input_ids=input_ids, labels=labels,attention_mask=mask)\n","            \n","            \n","            \n","            loss = outputs.loss\n","            \n","#           loss = loss / (FLAGS['GRAD_ACCUMULATION'] + scheduler.get_last_lr()[0]) # my touch for grad_norm\n","\n","\n","\n","            if (step + 1) % FLAGS['LOGGING_STEPS'] == 0:\n","                print(f'loss: {loss.item()}, time: {test_utils.now()}, step: {step+1}')\n","            if __wandb__:\n","                wandb.log({\n","                'Learning_rate': optimizer.param_groups[0]['lr'],\n","                'train_loss': loss.item(),\n","                'train_step': step + 1 + ((epoch-1) * FLAGS[\"STEPS\"]),\n","                        })\n","            \n","            loss.requires_grad_()\n","            loss.backward()\n","            optimizer.step()\n","            scheduler.step()\n","            xm.mark_step()\n","            optimizer.zero_grad()\n","            \n","        xm.master_print('Epoch {} train end {}'.format(epoch, test_utils.now()))\n","            \n","            \n","        \n","        \n","        \n","        \n","        ### Validation - Test\n","        model.eval()\n","        total_loss = 0.0\n","        total_steps = 0\n","        with torch.no_grad():\n","            for step, batch in enumerate(testing_loader):\n","                input_ids = batch[\"input_ids\"].to(device)\n","                labels = batch[\"labels\"].to(device)\n","                xs.mark_sharding(input_ids, mesh, (0, 1))\n","                xs.mark_sharding(labels, mesh, (0, 1))\n","                outputs = model(input_ids=input_ids, labels=labels)\n","                loss = outputs.loss\n","                total_loss += loss.item()\n","                if (step + 1) % FLAGS['LOGGING_STEPS'] == 0:\n","                    print(f'Testing Loss : {loss.item()}, time: {test_utils.now()}, step: {step+1}')\n","                total_steps += 1\n","\n","        average_loss = total_loss / total_steps\n","        xm.master_print('----- End Time -> {} ----- Total Validation Steps -> {} ----  Total Validation Loss -> {:.4f}'.format(test_utils.now(), total_steps , average_loss))\n","#         "]},{"cell_type":"markdown","metadata":{},"source":["**12 Mins to Train on 4k**"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-02-15T18:27:57.987963Z","iopub.status.busy":"2024-02-15T18:27:57.987235Z","iopub.status.idle":"2024-02-15T18:35:23.649407Z","shell.execute_reply":"2024-02-15T18:35:23.648588Z","shell.execute_reply.started":"2024-02-15T18:27:57.987922Z"},"trusted":true},"outputs":[{"data":{"text/html":["Finishing last run (ID:uxfhyy8m) before initializing another..."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["wandb: WARNING No program path found when generating artifact job source for a non-colab notebook run. See https://docs.wandb.ai/guides/launch/create-job\n","wandb: WARNING Source type is set to 'artifact' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"]},{"data":{"text/html":["W&B sync reduced upload amount by 18.7%             "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">dedicated-crush-1</strong> at: <a href='https://wandb.ai/fhai50032/Silver-Pulse/runs/uxfhyy8m' target=\"_blank\">https://wandb.ai/fhai50032/Silver-Pulse/runs/uxfhyy8m</a><br/>Synced 4 W&B file(s), 0 media file(s), 4 artifact file(s) and 1 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20240215_182639-uxfhyy8m/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Successfully finished last run (ID:uxfhyy8m). Initializing new run:<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.16.3"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20240215_182757-a3xbij39</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/fhai50032/Silver-Pulse/runs/a3xbij39' target=\"_blank\">fragrant-etchings-2</a></strong> to <a href='https://wandb.ai/fhai50032/Silver-Pulse' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/fhai50032/Silver-Pulse' target=\"_blank\">https://wandb.ai/fhai50032/Silver-Pulse</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/fhai50032/Silver-Pulse/runs/a3xbij39' target=\"_blank\">https://wandb.ai/fhai50032/Silver-Pulse/runs/a3xbij39</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 1 train begin 18:28:04\n","loss: 6.487453460693359, time: 18:28:26, step: 1\n","loss: 6.957941055297852, time: 18:30:38, step: 2\n","loss: 5.957518100738525, time: 18:32:12, step: 3\n","loss: 5.705552577972412, time: 18:32:14, step: 4\n","loss: 5.225892543792725, time: 18:32:16, step: 5\n","loss: 5.756276607513428, time: 18:32:18, step: 6\n","loss: 4.587933540344238, time: 18:32:20, step: 7\n","loss: 4.767673015594482, time: 18:32:22, step: 8\n","loss: 5.576902866363525, time: 18:32:24, step: 9\n","loss: 4.731902599334717, time: 18:32:26, step: 10\n","loss: 4.955680847167969, time: 18:32:28, step: 11\n","loss: 5.193166255950928, time: 18:32:30, step: 12\n","loss: 4.506847381591797, time: 18:32:32, step: 13\n","loss: 5.372564315795898, time: 18:32:33, step: 14\n","loss: 4.4137349128723145, time: 18:32:35, step: 15\n","loss: 4.066483020782471, time: 18:32:37, step: 16\n","loss: 4.605711460113525, time: 18:32:39, step: 17\n","loss: 4.2736968994140625, time: 18:32:41, step: 18\n","loss: 4.799393177032471, time: 18:32:43, step: 19\n","loss: 4.200766563415527, time: 18:32:45, step: 20\n","loss: 4.448278427124023, time: 18:32:47, step: 21\n","loss: 3.8512604236602783, time: 18:32:49, step: 22\n","loss: 4.389097213745117, time: 18:32:51, step: 23\n","loss: 3.561995029449463, time: 18:32:53, step: 24\n","loss: 4.087070941925049, time: 18:32:55, step: 25\n","loss: 4.07573938369751, time: 18:32:57, step: 26\n","loss: 4.06845760345459, time: 18:32:59, step: 27\n","loss: 4.330987930297852, time: 18:33:01, step: 28\n","loss: 3.7817392349243164, time: 18:33:03, step: 29\n","loss: 4.053389072418213, time: 18:33:05, step: 30\n","loss: 4.325424671173096, time: 18:33:07, step: 31\n","loss: 4.066512107849121, time: 18:33:26, step: 32\n","Epoch 1 train end 18:35:03\n","Testing Loss : 5.845332145690918, time: 18:35:20, step: 1\n","----- End Time -> 18:35:20 ----- Total Validation Steps -> 1 ----  Total Validation Loss -> 5.8453\n"]},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Learning_rate</td><td>▁▃▆███▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▁▁</td></tr><tr><td>train_loss</td><td>▇█▆▅▄▆▃▃▅▃▄▄▃▅▃▂▃▂▄▂▃▂▃▁▂▂▂▃▁▂▃▂</td></tr><tr><td>train_step</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Learning_rate</td><td>0.0</td></tr><tr><td>train_loss</td><td>4.06651</td></tr><tr><td>train_step</td><td>64</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">fragrant-etchings-2</strong> at: <a href='https://wandb.ai/fhai50032/Silver-Pulse/runs/a3xbij39' target=\"_blank\">https://wandb.ai/fhai50032/Silver-Pulse/runs/a3xbij39</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20240215_182757-a3xbij39/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["train(FLAGS)\n","if FLAGS['WANDB']:\n","    wandb.finish()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print('Loading the model on CPU')\n","START=time.time()\n","model = model.cpu()\n","print(f\"Loaded model on cpu in {time.time()-START} seconds \")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from huggingface_hub import login\n","login(\"<HF_WRITE_TOKEN>\") ##\"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\"\n","model.push_to_hub(\n","    SAVED_MODEL, \n","    tokenizer=tokenizer,\n","    safe_serialization=True,\n","    private=True,\n","    create_pr=True,\n","    max_shard_size=\"3GB\", \n","    )\n","tokenizer.push_to_hub(\n","    SAVED_MODEL,\n","    private=True, \n","    \n","    )"]}],"metadata":{"kaggle":{"accelerator":"tpu1vmV38","dataSources":[],"dockerImageVersionId":30647,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
